{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import fnmatch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from patch_functions import *\n",
    "from loss_functions import *\n",
    "from dataset_functions import *\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the matlab file to tensor and save them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training folder\n",
    "masks = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\training\\masks\")]\n",
    "# Test folder \n",
    "masks = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\test\\masks\")]\n",
    "\n",
    "for mask in masks:\n",
    "    mask_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\test\\masks' + '/' + mask\n",
    "    matfile = scipy.io.loadmat(mask_path)\n",
    "    # Extract the mask from it \n",
    "    number = mask[6:-4]\n",
    "    mask = matfile['BW'+ number]\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "    tensor_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\test\\masks' + '/image' + str(number) + '.pt'\n",
    "    torch.save(mask_tensor, tensor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull trasformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming from PIL to Tensor\n",
    "transform1 = transforms.ToTensor()\n",
    "\n",
    "# Transforming from Tensor to PIL\n",
    "transform2 = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Model: con queste due righe di codice Ã¨ possibile definire il modello \n",
    "model = torch.hub.load('ultralytics/yolov3', 'yolov3', autoshape = False)  # or yolov3-spp, yolov3-tiny, custom\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define the model without the autoshape = False \n",
    "model_lab = torch.hub.load('ultralytics/yolov3', 'yolov3')  # or yolov3-spp, yolov3-tiny, custom\n",
    "model_lab.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/home/andread98/yolov3/inria/INRIAPerson/Train/pos'\n",
    "images = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "\n",
    "for image in images:\n",
    "    # I consider the txt file path \n",
    "    img_path = images_path + '/' + image\n",
    "    # Get the results \n",
    "    results = model_lab(img_path)\n",
    "    # Trasform the yolo labels into a numpy array to save them\n",
    "    array = results.xywhn[0].cpu().numpy()\n",
    "    # Select only the object that are people\n",
    "    array = array[array[:,-1] == 0]\n",
    "    # Classes in the first column \n",
    "    permutation = [1, 2, 3, 4, 5, 0]\n",
    "    idx = np.empty_like(permutation)\n",
    "    idx[permutation] = np.arange(len(permutation))\n",
    "    array[:] = array[:, idx]\n",
    "    # Remove the last column\n",
    "    array = array[:,:-1]\n",
    "    # Save the array \n",
    "    np.savetxt(images_path + '/yolo-labels/' + image[0:-4] + '.txt', array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images in the training set\n",
    "\n",
    "list = os.listdir('/home/andread98/yolov3/inria/INRIAPerson/Train/pos')\n",
    "print(len(list)-1)\n",
    "\n",
    "# Number of images in the training set\n",
    "\n",
    "list = os.listdir('/home/andread98/yolov3/inria/INRIAPerson/Train/pos/yolo-labels')\n",
    "print(len(list))\n",
    "\n",
    "# The two quantities of elements match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate all the image with a most 2 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/home/andread98/yolov3/inria/INRIAPerson/Train/pos'\n",
    "images = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "\n",
    "for image in images:\n",
    "    # I consider the txt file path \n",
    "    img_path = images_path + '/' + image\n",
    "    destination = '/home/andread98/yolov3/dataset/' + image\n",
    "    # Get the results \n",
    "    results = model_lab(img_path)\n",
    "    # Trasform the yolo labels into a numpy array to save them\n",
    "    array = results.xywhn[0].cpu().numpy()\n",
    "    # Select only the object that are people\n",
    "    array = array[array[:,-1] == 0]\n",
    "    # Classes in the first column \n",
    "    permutation = [1, 2, 3, 4, 5, 0]\n",
    "    idx = np.empty_like(permutation)\n",
    "    idx[permutation] = np.arange(len(permutation))\n",
    "    array[:] = array[:, idx]\n",
    "    # Remove the last column\n",
    "    array = array[:,:-1]\n",
    "    if array.shape[0] <= 2:\n",
    "        # Save the array \n",
    "        np.savetxt('/home/andread98/yolov3/dataset/yolo-labels/' + image[0:-4] + '.txt', array)\n",
    "        # Save a copy of the image in the right folder\n",
    "        shutil.copy(img_path, destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images in the training set\n",
    "\n",
    "list = os.listdir('/home/andread98/yolov3/dataset')\n",
    "print(len(list)-1)\n",
    "\n",
    "# Number of images in the training set\n",
    "\n",
    "list = os.listdir('/home/andread98/yolov3/dataset/yolo-labels')\n",
    "print(len(list))\n",
    "\n",
    "# The two quantities of elements match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the name to the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path = '/home/andread98/yolov3/data_mask/mask'\n",
    "masks = [f for f in os.listdir(masks_path) if f.endswith('.mat')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in masks:\n",
    "    origin = '/home/andread98/yolov3/data_mask/mask/' + mask\n",
    "    destination ='/home/andread98/yolov3/data_mask/masks/image' + str(mask[6:-4]) +'.mat'\n",
    "    shutil.copy(origin, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tensor instead of the .mat file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path = '/home/andread98/yolov3/data_mask/masks'\n",
    "masks = [f for f in os.listdir(masks_path) if f.endswith('.mat')]\n",
    "\n",
    "for mask in masks:\n",
    "    number = mask[5:-4]\n",
    "    matfile = scipy.io.loadmat('/home/andread98/yolov3/data_mask/masks/image' + number + '.mat')\n",
    "    # Extract the mask from it \n",
    "    mask = matfile['BW'+ number]\n",
    "    tensor = torch.from_numpy(mask)\n",
    "    save_directory = '/home/andread98/yolov3/data_mask/mask/image'  + number + '.pt'\n",
    "    torch.save(tensor, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between model() and model().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# create the image\n",
    "image = torch.rand((1,3,640,640))\n",
    "image = image.to(device)\n",
    "\n",
    "# model without eval\n",
    "model = torch.hub.load('ultralytics/yolov3', 'yolov3', autoshape = False)  # or yolov3-spp, yolov3-tiny, custom\n",
    "\n",
    "output = model(image)\n",
    "print(output)\n",
    "\n",
    "# model with eval\n",
    "model = model.eval()\n",
    "output = model(image)\n",
    "print(output)\n",
    "\n",
    "#They are different, I should use the eval. We don't want to train the network\n",
    "#But the parameters of the patch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset and convert the images to tensors \n",
    "\n",
    "training_data = datasets.VOCDetection(\n",
    "    root= \"data\",\n",
    "    year = \"2007\",\n",
    "    download = False,\n",
    "    transform = ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                training_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the output of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/home/andread98/yolov3/inria/INRIAPerson/Train/pos\"\n",
    "batch_size = 6\n",
    "img_size=640\n",
    "\n",
    "# Create the train_loader that will be used for the training routine \n",
    "# Setting the batch and the image directory\n",
    "\n",
    "train_loader_2 = torch.utils.data.DataLoader(\n",
    "                InriaDataset(img_dir, img_size, shuffle=True),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader_2):\n",
    "    image = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = torch.hub.load('ultralytics/yolov3', 'yolov3', autoshape = False)  # or yolov3-spp, yolov3-tiny, custom\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader_2):\n",
    "    image = data.cuda()\n",
    "    print(image.dtype)\n",
    "    outputs = model(image)\n",
    "    print('Len output: ',len(outputs))\n",
    "    print(outputs[0].shape)\n",
    "    print(outputs[1][0].shape)\n",
    "    print(outputs[1][1].shape)\n",
    "    print(outputs[1][2].shape)\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to obtain the first tensor given the others\n",
    "Note that the coordinates of the BB still don't make sense, I need to understand how to compute them starting form the other outputs. \n",
    "But, to define the loss the BB coordinates are not necessary. The BB coordinated can be found with the basic version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "final_true = outputs[0]\n",
    "yolo_output1 = outputs[1][0]\n",
    "print(torch.reshape(yolo_output1, (1,3*80*80,85))[0,0,:])\n",
    "yolo_output1 = torch.reshape(yolo_output1, (1,3*80*80,85)).sigmoid()\n",
    "print(yolo_output1)\n",
    "\n",
    "yolo_output2 = outputs[1][1]\n",
    "yolo_output2 = torch.reshape(yolo_output2, (1,3*40*40,85)).sigmoid()\n",
    "\n",
    "yolo_output3 = outputs[1][2]\n",
    "yolo_output3 = torch.reshape(yolo_output3, (1,3*20*20,85)).sigmoid()\n",
    "\n",
    "final_tensor = torch.cat((yolo_output1,yolo_output2,yolo_output3), dim=1)\n",
    "print(final_true.shape)\n",
    "final_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(final_true[0,0,:])\n",
    "print(final_tensor[0,0,:])\n",
    "print(torch.reshape(yolo_output1, (1,3*80*80,85)))\n",
    "# Same probabilities!!\n",
    "print(final_true[0,0,4:]==final_tensor[0,0,4:])\n",
    "print(final_true[0,22999,4:]==final_tensor[0,22999,4:])\n",
    "print(final_true[0,25199,4:]==final_tensor[0,25199,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works, so we can use the other tensors to compute the backward step\n",
    "torch.sum(final_tensor).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):   \n",
    "    # Patch definition\n",
    "    patch = torch.full((1, 3, 100, 100), 0.5)\n",
    "    patch = patch.to(device)\n",
    "    patch.requires_grad_(True)\n",
    "\n",
    "    # Black image definition\n",
    "    black_image = torch.zeros((1,3,640,640))\n",
    "    black_image = black_image.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = new_loss_tprob(i).to(device)\n",
    "\n",
    "    # Patch applier function\n",
    "    apply_function = apply_patch().to(device)\n",
    "\n",
    "    # Optimizer definition\n",
    "    optimizer = optim.SGD([patch], lr = 0.03)\n",
    "\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    for epoch in range(50000):  # loop over the dataset multiple times\n",
    "        print('Iteration number: ', epoch)\n",
    "        \n",
    "        b_image = black_image.detach().clone()\n",
    "\n",
    "        # Applying the patch to the black-image\n",
    "        image = apply_function(patch,b_image)\n",
    "        \n",
    "        # forward of the model\n",
    "        outputs = model(image)\n",
    "        loss = torch.mean(-loss_function(outputs))\n",
    "        # print(loss)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Max image grad: ',torch.max(patch.grad))\n",
    "        print('Min image grad: ',torch.min(patch.grad))\n",
    "        patch.data.clamp_(0, 1)\n",
    "        print('Max image: ',torch.max(patch))\n",
    "        print('Min image: ',torch.min(patch))        # Keep the patch in range (0,1)\n",
    "        print('Loss value: ',loss)\n",
    "    \n",
    "    name = '/home/andread98/yolov3/Results/SGD_class' + str(i) + '.pt'\n",
    "    torch.save(patch, name)\n",
    "\n",
    "    # image_PIL = transform2(patch.squeeze(0))\n",
    "    # image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load the patch\n",
    "\n",
    "# torch.save(tensor_name, 'tensor_name.pt')\n",
    "# tensor_name = torch.load('tensor_name.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.zeros((3,200,200))\n",
    "tensor_PIL = transform2(tensor)\n",
    "tensor_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the crafted Patch\n",
    "Let's see if the patch works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_test = torch.hub.load('ultralytics/yolov3', 'yolov3')  # or yolov3-spp, yolov3-tiny, custom\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = torch.load('patch.pt')\n",
    "black = torch.zeros((1,3,640,640))\n",
    "apply_function = apply_patch()\n",
    "image = apply_function(patch,black)\n",
    "image_PIL = transform2(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_test(image_PIL)\n",
    "outputs.show()\n",
    "print(outputs.xywhn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to apply the patch on all the people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset_functions import *\n",
    "from patch_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/home/andread98/yolov3/inria/INRIAPerson/Train/pos'\n",
    "labels_path = '/home/andread98/yolov3/inria/INRIAPerson/Train/pos/yolo-labels'\n",
    "max_lab = 20\n",
    "img_size = 614\n",
    "batch_size = 6\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                InriaDataset(images_path, labels_path, max_lab, img_size,\n",
    "                             shuffle=True),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Definizione dei parametri\n",
    "a = torch.tensor(0.70)\n",
    "a.requires_grad_(True)\n",
    "b = torch.tensor(0.3)\n",
    "b.requires_grad_(True)\n",
    "color1 = torch.tensor([1.0,0.0,0.0])\n",
    "color1.requires_grad_(True)\n",
    "color2 = torch.tensor([0.0,0.0,1.0])\n",
    "color2.requires_grad_(True)\n",
    "\n",
    "params = [a, color1, color2]\n",
    "\n",
    "gen_function = Fractal_Patch_Generator(20, 300, 3,Tile_Creator_Square,Mask_Creator,2)\n",
    "\n",
    "gen_function.populate(params)\n",
    "adv_patch = gen_function.application()\n",
    "adv_patch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_PIL = transform2(adv_patch)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_applier = PatchApplier().to(device)\n",
    "patch_transformer = PatchTransformer().to(device)\n",
    "img_size = 614\n",
    "\n",
    "for i_batch, (img_batch, lab_batch) in enumerate(train_loader):\n",
    "    print(img_batch.shape)\n",
    "    img_batch = img_batch.to(device)\n",
    "    lab_batch = lab_batch.to(device)\n",
    "    adv_patch = adv_patch.to(device)\n",
    "    # print(lab_batch)\n",
    "    adv_batch_t = patch_transformer(adv_patch, lab_batch, img_size, do_rotate=True, rand_loc=False)\n",
    "    p_img_batch = patch_applier(img_batch, adv_batch_t)\n",
    "    p_img_batch = F.interpolate(p_img_batch, (img_size, img_size))\n",
    "                            \n",
    "    if i_batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = p_img_batch[1,:,:,:]\n",
    "\n",
    "image_PIL = transform2(image)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working of the mask, how to create a dataloader with image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_functions import *\n",
    "\n",
    "img_dir = '/home/andread98/yolov3/MyWork/data_mask'\n",
    "mask_dir = '/home/andread98/yolov3/MyWork/data_mask/mask'\n",
    "img_size = 640\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                VOCmask(img_dir, mask_dir, img_size,\n",
    "                             shuffle=True),\n",
    "                batch_size=2,\n",
    "                shuffle=True,\n",
    "                num_workers=10)\n",
    "\n",
    "dataset = VOCmask(img_dir, mask_dir, img_size,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, (img_batch, mask_batch) in enumerate(train_loader):\n",
    "    images = img_batch\n",
    "    print(images.shape)\n",
    "    masks = mask_batch\n",
    "    print(masks.shape)\n",
    "\n",
    "    if True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov3', 'yolov3', autoshape = False)  # or yolov3-spp, yolov3-tiny, custom\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "masks = masks.to(device)\n",
    "results = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = torch.rand((3,640,640)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_images = images*(1-masks) + patch*masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_image = attacked_images[0].squeeze(0)\n",
    "att_image_PIL = transform2(att_image)\n",
    "att_image_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perlin Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perlin_noise import Perlin_Noise_Creator, Inverted_Perlin_Noise_Creator\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = torch.tensor([0.5,0.2,0.7])\n",
    "dim=640\n",
    "\n",
    "pn_tensor = Perlin_Noise_Creator(dim,color1)\n",
    "image_PIL = transform2(pn_tensor)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = torch.tensor([0.5,0.2,0.7])\n",
    "dim=640\n",
    "\n",
    "pn_tensor = Inverted_Perlin_Noise_Creator(dim,color1)\n",
    "image_PIL = transform2(pn_tensor)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from median_pool import MedianPool2d\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial version\n",
    "class Tile_Creator_Square(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, dim, params):\n",
    "        # Tensore delle distanze \n",
    "        image = torch.zeros((3,dim,dim))\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                image[:,i,j] = torch.max(torch.abs(torch.tensor(i-(dim-1)/2)/params[0]),torch.abs(torch.tensor(j-(dim-1)/2))/params[0]) - dim/2\n",
    "        coeff = image.sigmoid()\n",
    "\n",
    "        # Creation of the colors tensors\n",
    "        color1_image = params[1].unsqueeze(-1).unsqueeze(-1)\n",
    "        color1_image = color1_image.expand(-1, dim, dim)\n",
    "\n",
    "        return (1-coeff)*color1_image, (1-coeff)\n",
    "\n",
    "    def Params_Creator(self):\n",
    "        a = torch.tensor(0.50)\n",
    "        a.requires_grad_(True)\n",
    "        color1 = torch.tensor([0,1.0,0])\n",
    "        color1.requires_grad_(True)\n",
    "\n",
    "        params = [a, color1]\n",
    "        return params\n",
    "\n",
    "    def Params_Clamp(self,params):\n",
    "        params[0].data.clamp_(0, factor)\n",
    "        params[1].data.clamp_(0, 1)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version\n",
    "class Tile_Creator_Square(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, dim, params):\n",
    "        # Tensore delle distanze \n",
    "        image = torch.zeros((3,dim,dim))\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                image[:,i,j] = torch.max(torch.abs(torch.tensor(i-(dim-1)/2)/params[0]),torch.abs(torch.tensor(j-(dim-1)/2))/params[0]) - dim/2\n",
    "        coeff = image.sigmoid()\n",
    "\n",
    "        # Creation of the colors tensors\n",
    "        color1_image = params[1].unsqueeze(-1).unsqueeze(-1)\n",
    "        color1_image = color1_image.expand(-1, dim, dim)\n",
    "\n",
    "        color2_image = params[2].unsqueeze(-1).unsqueeze(-1)\n",
    "        color2_image = color2_image.expand(-1, dim, dim)\n",
    "\n",
    "        return coeff*color1_image + (1-coeff)*color2_image, color1_image, (1-coeff)\n",
    "\n",
    "    def Params_Creator(self):\n",
    "        a = torch.tensor(0.50)\n",
    "        a.requires_grad_(True)\n",
    "        color1 = torch.tensor([1.0,0,0])\n",
    "        color1.requires_grad_(True)\n",
    "        color2 = torch.tensor([0, 1.0, 0])\n",
    "        color2.requires_grad_(True)\n",
    "\n",
    "        params = [a, color1, color2]\n",
    "        return params\n",
    "\n",
    "    def Params_Clamp(self,params):\n",
    "        params[0].data.clamp_(0, factor)\n",
    "        params[1].data.clamp_(0, 1)\n",
    "        params[2].data.clamp_(0, 1)\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def Give_Color_Perlin(self,params):\n",
    "        return params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_function = Tile_Creator_Square()\n",
    "params = tile_function.Params_Creator()\n",
    "image, _, mask = tile_function(150,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.rand((3,150,150))\n",
    "tot = mask*image + (1-mask)*random_tensor\n",
    "image_PIL = transform2(tot)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial version\n",
    "class Fractal_Patch_Generator_Ghost(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_patch, dim_image, max_dim, tile_class, angle_type):\n",
    "        super(Fractal_Patch_Generator_Ghost, self).__init__()\n",
    "        #Dimension of the patch, smallest component of the attack\n",
    "        self.dim_patch = dim_patch \n",
    "        #Dimension of the image we want to create\n",
    "        self.dim_image = dim_image\n",
    "        #Max dimension for the patch\n",
    "        self.max_dim = max_dim\n",
    "        #Tile function\n",
    "        self.tile_class = tile_class\n",
    "        #Angle Type\n",
    "        self.angle_type = angle_type\n",
    "\n",
    "        #Check if they are compatible\n",
    "        if self.dim_image % self.dim_patch != 0:\n",
    "            raise Exception('Patch and Image do not have compatible dimensions. Please select the image as a multiple of patch.')\n",
    "\n",
    "        # How many lines in the grid?\n",
    "        self.dim_grid = int(self.dim_image/self.dim_patch)\n",
    "    \n",
    "    def populate(self, params):\n",
    "        self.patches = []\n",
    "        self.masks = []\n",
    "\n",
    "        for i in range(1,self.max_dim+1):\n",
    "            patch, _, mask = self.tile_class(self.dim_patch*i, params)\n",
    "            print(mask.shape)\n",
    "            self.patches.append(patch)\n",
    "            self.masks.append(mask)\n",
    "            \n",
    "    \n",
    "    def application(self):\n",
    "        #Creation of the complete image\n",
    "        self.image = torch.rand((3,self.dim_image,self.dim_image))\n",
    "        #Creation of the complete mask\n",
    "        self.complete_mask = torch.zeros((3,self.dim_image,self.dim_image))\n",
    "        #Creation of the bool vector \n",
    "        self.bool_matrix = np.ones((self.dim_grid,self.dim_grid), dtype=int)\n",
    "        #Creation of the index vector\n",
    "        self.index_vector = np.arange(0,(self.dim_grid**2))\n",
    "        #Creation of the shuffled version\n",
    "        shuffled_index_vector = np.random.choice(self.index_vector, size=self.dim_grid**2, replace=False)\n",
    "        # #PIL image of the patch\n",
    "        # patch_PIL = transform2(patch)\n",
    "\n",
    "        for index in shuffled_index_vector:\n",
    "            #Translate the index in coordinates\n",
    "            i = int(index/self.dim_grid) #row\n",
    "            j = index%self.dim_grid      #column\n",
    "\n",
    "            #Check if the corner is still available\n",
    "            if self.bool_matrix[i][j]:\n",
    "                av_dim = self.available_dimensions(i,j)\n",
    "                \n",
    "                #Choose randomly the dimension\n",
    "                chosen_dim = int(np.random.choice(av_dim, 1))\n",
    "                # print(chosen_dim)\n",
    "\n",
    "                #Change the bool in the bool matrix in False, no longer available\n",
    "                self.bool_matrix[i:i+chosen_dim,j:j+chosen_dim] = 0\n",
    "\n",
    "                if self.angle_type == 0:\n",
    "                    self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = self.patches[chosen_dim-1]\n",
    "\n",
    "                elif self.angle_type == 1:\n",
    "                    angle = random.choice([0,90,180,270])\n",
    "                    #Apply the patch to the image\n",
    "                    self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = TF.rotate(self.patches[chosen_dim-1],angle)\n",
    "\n",
    "                else:\n",
    "                    #Select the angle\n",
    "                    angle = random.uniform(0,360)\n",
    "                    #Rotate\n",
    "                    out = TF.rotate(self.patches[chosen_dim-1], angle)\n",
    "                    out2 = TF.rotate(self.masks[chosen_dim-1], angle)\n",
    "                    # color = self.ex_colors[chosen_dim-1]\n",
    "                    # #Color in the angles\n",
    "                    # mask = self.masks[chosen_dim-1]\n",
    "                    # out[mask] = color[mask]\n",
    "                    # #Apply the patch to the image\n",
    "                    self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = out\n",
    "                    self.complete_mask[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = out2\n",
    "\n",
    "        return self.image, self.complete_mask\n",
    "        \n",
    "    def available_dimensions(self,i,j):\n",
    "        #It is always possible to put the smallest version of the patch\n",
    "        av_dim = [1]\n",
    "\n",
    "        for dim in range(2, self.max_dim+1):\n",
    "            sub_matrix = self.bool_matrix[i:i+dim,j:j+dim]\n",
    "            if np.sum(sub_matrix) == dim**2:\n",
    "                av_dim.append(dim)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return av_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version\n",
    "class Fractal_Patch_Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_patch, dim_image, max_dim, tile_class, angle_type, BackgroundStyle, mask_function = None):\n",
    "        super(Fractal_Patch_Generator, self).__init__()\n",
    "        #Dimension of the patch, smallest component of the attack\n",
    "        self.dim_patch = dim_patch \n",
    "        #Dimension of the image we want to create\n",
    "        self.dim_image = dim_image\n",
    "        #Max dimension for the patch\n",
    "        self.max_dim = max_dim\n",
    "        #Tile function\n",
    "        self.tile_class = tile_class\n",
    "        #Mask function\n",
    "        self.mask_function = mask_function\n",
    "        #Angle Type\n",
    "        self.angle_type = angle_type\n",
    "        #Background Style\n",
    "        self.BackgroundStyle = BackgroundStyle\n",
    "\n",
    "        #Check if they are compatible\n",
    "        if self.dim_image % self.dim_patch != 0:\n",
    "            raise Exception('Patch and Image do not have compatible dimensions. Please select the image as a multiple of patch.')\n",
    "\n",
    "        # How many lines in the grid?\n",
    "        self.dim_grid = int(self.dim_image/self.dim_patch)\n",
    "    \n",
    "    def populate(self, params):\n",
    "\n",
    "        if self.BackgroundStyle == 0: # Normal situation with a plain color\n",
    "            self.patches = []\n",
    "            self.ex_colors = []\n",
    "            self.masks = []\n",
    "\n",
    "            for i in range(1,self.max_dim+1):\n",
    "                patch, ex_color, _ = self.tile_class(self.dim_patch*i, params)\n",
    "                mask = self.mask_function(self.dim_patch*i)\n",
    "\n",
    "                self.patches.append(patch)\n",
    "                self.ex_colors.append(ex_color)\n",
    "                self.masks.append(mask)\n",
    "        \n",
    "        else: # Using the perlin noise in the background\n",
    "            self.patches = []\n",
    "            self.masks = []\n",
    "            # Creation of the perlin noise with the function in perlin_noise.py\n",
    "            if self.BackgroundStyle == 1:\n",
    "                self.perlin_noise = Perlin_Noise_Creator(self.dim_image, self.tile_class.Give_Color_Perlin(params))\n",
    "            if self.BackgroundStyle == 2:\n",
    "                self.perlin_noise = Inverted_Perlin_Noise_Creator(self.dim_image, self.tile_class.Give_Color_Perlin(params))\n",
    "\n",
    "            for i in range(1,self.max_dim+1):\n",
    "                patch, _, mask = self.tile_class(self.dim_patch*i, params)\n",
    "                self.patches.append(patch)\n",
    "                self.masks.append(mask)            \n",
    "    \n",
    "    def application(self):\n",
    "        #Creation of the complete image\n",
    "        self.image = torch.rand((3,self.dim_image,self.dim_image))\n",
    "        #Creation of the bool vector \n",
    "        self.bool_matrix = np.ones((self.dim_grid,self.dim_grid), dtype=int)\n",
    "        #Creation of the index vector\n",
    "        self.index_vector = np.arange(0,(self.dim_grid**2))\n",
    "        #Creation of the shuffled version\n",
    "        shuffled_index_vector = np.random.choice(self.index_vector, size=self.dim_grid**2, replace=False)\n",
    "        self.complete_mask = torch.zeros((3,self.dim_image,self.dim_image))\n",
    "\n",
    "        for index in shuffled_index_vector:\n",
    "            #Translate the index in coordinates\n",
    "            i = int(index/self.dim_grid) #row\n",
    "            j = index%self.dim_grid      #column\n",
    "\n",
    "            #Check if the corner is still available\n",
    "            if self.bool_matrix[i][j]:\n",
    "                av_dim = self.available_dimensions(i,j)\n",
    "                \n",
    "                #Choose randomly the dimension\n",
    "                chosen_dim = int(np.random.choice(av_dim, 1))\n",
    "                # print(chosen_dim)\n",
    "\n",
    "                #Change the bool in the bool matrix in False, no longer available\n",
    "                self.bool_matrix[i:i+chosen_dim,j:j+chosen_dim] = 0\n",
    "\n",
    "                if self.angle_type == 0:\n",
    "                    self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = self.patches[chosen_dim-1]\n",
    "\n",
    "                elif self.angle_type == 1:\n",
    "                    angle = random.choice([0,90,180,270])\n",
    "                    #Apply the patch to the image\n",
    "                    self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = TF.rotate(self.patches[chosen_dim-1],angle)\n",
    "\n",
    "                else:\n",
    "                    #Select the angle\n",
    "                    angle = random.uniform(0,360)\n",
    "\n",
    "                    \n",
    "\n",
    "                    if self.BackgroundStyle == 0: # Normal case\n",
    "                        #Rotate\n",
    "                        out = TF.rotate(self.patches[chosen_dim-1], angle)\n",
    "                        # Getting the background color\n",
    "                        color = self.ex_colors[chosen_dim-1]\n",
    "\n",
    "                        # Color in the angles\n",
    "                        mask = self.masks[chosen_dim-1]\n",
    "                        out[mask] = color[mask]\n",
    "\n",
    "                        # Apply the patch to the image\n",
    "                        self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = out\n",
    "\n",
    "                    else: # Perlin Noise\n",
    "                        #Rotate\n",
    "                        out = TF.rotate(self.patches[chosen_dim-1], angle)\n",
    "                        # Rotate the mask\n",
    "                        out2 = TF.rotate(self.masks[chosen_dim-1], angle)\n",
    "                        \n",
    "                        # Apply the patch to the image  \n",
    "                        self.image[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = out\n",
    "                        \n",
    "                        # Tiling all the tile together to create to complete mask \n",
    "                        self.complete_mask[:,i*self.dim_patch:(i+chosen_dim)*self.dim_patch,j*self.dim_patch:(j+chosen_dim)*self.dim_patch] = out2\n",
    "\n",
    "        if self.BackgroundStyle == 0:\n",
    "            return self.image\n",
    "        elif self.BackgroundStyle == 1 or self.BackgroundStyle == 2:\n",
    "            self.image = self.complete_mask*self.image + (1-self.complete_mask)*self.perlin_noise\n",
    "            return self.image\n",
    "        else:\n",
    "            return  self.complete_mask*self.image, self.complete_mask\n",
    "        \n",
    "    def available_dimensions(self,i,j):\n",
    "        #It is always possible to put the smallest version of the patch\n",
    "        av_dim = [1]\n",
    "\n",
    "        for dim in range(2, self.max_dim+1):\n",
    "            sub_matrix = self.bool_matrix[i:i+dim,j:j+dim]\n",
    "            if np.sum(sub_matrix) == dim**2:\n",
    "                av_dim.append(dim)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return av_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mask_Creator(dim):\n",
    "    mask = torch.ones((3,dim,dim), dtype=torch.bool)\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if torch.sqrt(torch.tensor((i-(dim-1)/2)**2 + (j-(dim-1)/2)**2)) - (dim/2) < 0:\n",
    "                mask[:,i,j] = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_function = Tile_Creator_Square()\n",
    "dim_tile = 16\n",
    "dim_patch = 640 \n",
    "mul_fact = 4\n",
    "angle_type = 2\n",
    "BackgroundStyle = 0\n",
    "params = tile_function.Params_Creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_function = Fractal_Patch_Generator(dim_tile, dim_patch, mul_fact, tile_function, angle_type, BackgroundStyle, Mask_Creator)\n",
    "gen_function.populate(params)\n",
    "image = gen_function.application()\n",
    "image_PIL = transform2(image)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_function = Tile_Creator_Square()\n",
    "dim_tile = 16\n",
    "dim_patch = 640 \n",
    "mul_fact = 4\n",
    "angle_type = 2\n",
    "BackgroundStyle = 3\n",
    "params = tile_function.Params_Creator()\n",
    "gen_function = Fractal_Patch_Generator(dim_tile, dim_patch, mul_fact, tile_function, angle_type, BackgroundStyle, Mask_Creator)\n",
    "gen_function.populate(params)\n",
    "attack, mask_attack = gen_function.application()\n",
    "image_PIL = transform2(attack)\n",
    "image_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ghost Patch (just the shape with no background)\n",
    "\n",
    "#### Patch applier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original version\n",
    "class PatchApplierMask(nn.Module):\n",
    "    \"\"\"PatchApplier: applies adversarial patches to images.\n",
    "\n",
    "    Module providing the functionality necessary to apply a patch to all detections in all images in the batch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PatchApplierMask, self).__init__()\n",
    "\n",
    "    def forward(self, img_batch, masks_batch, adv_patch):\n",
    "        att_img_batch = img_batch*(1-masks_batch) + adv_patch*masks_batch\n",
    "        return att_img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New version\n",
    "class PatchApplierMask_ghost(nn.Module):\n",
    "    \"\"\"PatchApplier: applies adversarial patches to images.\n",
    "\n",
    "    Module providing the functionality necessary to apply a patch to all detections in all images in the batch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PatchApplierMask_ghost, self).__init__()\n",
    "\n",
    "    def forward(self, img_batch, mask_attack, masks_batch, adv_patch):\n",
    "        att_img_batch = img_batch*((1-masks_batch) + masks_batch*(1-mask_attack))+ adv_patch*masks_batch*mask_attack\n",
    "        return att_img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patch_config_mask_test as patch_config_mask_test\n",
    "mode = 'test'\n",
    "config = patch_config_mask_test.patch_configs[mode]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\test'\n",
    "mask_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\yolov3\\MyWork\\dataset\\test\\masks'\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                            VOCmask(img_path, mask_path, 640,\n",
    "                                        shuffle=True),\n",
    "                            batch_size= 1,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_applier = PatchApplierMask_ghost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, (img_batch, masks_batch) in enumerate(test_loader):\n",
    "    image = img_batch\n",
    "    mask = masks_batch\n",
    "    attacked_img_batch = patch_applier(image, mask_attack,mask, attack)\n",
    "    img_PIL = transform2(attacked_img_batch.squeeze(0))\n",
    "    img_PIL.show()\n",
    "    if i_batch == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "path1 = '/home/andread98/yolov3/MyWork/txt_results/22-09-2022/*'\n",
    "path2 = '/home/andread98/yolov3/MyWork/SampleImages/22-09-2022/*'\n",
    "path3 = '/home/andread98/yolov3/MyWork/params_results/22-09-2022/*'\n",
    "paths = [path1, path2, path3]\n",
    "\n",
    "for path in paths:\n",
    "    files = glob.glob(path)\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/dataset/imaterialist-fashion-2019-FGVC6/test'\n",
    "\n",
    "images = [f for f in os.listdir('/dataset/imaterialist-fashion-2019-FGVC6/test')]\n",
    "\n",
    "for image in images:\n",
    "    image_path = '/dataset/imaterialist-fashion-2019-FGVC6/test/' + image\n",
    "    print(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission more shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_function = Tile_Creator_Circle()\n",
    "tile_function2 = Tile_Creator_Square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.zeros((3,64,64))\n",
    "params = tile_function.Params_Creator(64)\n",
    "tile, color, mask = tile_function(64,params)\n",
    "tensor_step1 = tensor*(1-mask) + tile*mask\n",
    "params = tile_function.Params_Creator(64)\n",
    "tile, color, mask = tile_function(64,params)\n",
    "tensor_step2 = tensor_step1*(1-mask) + tile*mask\n",
    "params = tile_function2.Params_Creator(64)\n",
    "tile, color, mask = tile_function2(64,params)\n",
    "tensor_step3 = tensor_step2*(1-mask) + tile*mask\n",
    "tensor_PIL = transform2(tensor_step3)\n",
    "tensor_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Model: con queste due righe di codice Ã¨ possibile definire il modello \n",
    "model = torch.hub.load('ultralytics/yolov3', 'yolov3', autoshape = False)  # or yolov3-spp, yolov3-tiny, custom\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the params in a list\n",
    "params = tile_function.Params_Creator(64)\n",
    "print(params)\n",
    "params2 = tile_function2.Params_Creator(64)\n",
    "print(params2)\n",
    "params_tot = params + params2\n",
    "\n",
    "tensor = torch.zeros((3,64,64))\n",
    "tile, color, mask = tile_function(64,params)\n",
    "tensor_step1 = tensor*(1-mask) + tile*mask\n",
    "tile, color, mask = tile_function2(64,params2)\n",
    "tensor_step2 = tensor_step1*(1-mask) + tile*mask\n",
    "tensor_final = tensor_step2.tile((10,10))\n",
    "print(tensor_final.shape)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(params_tot, learning_rate)\n",
    "\n",
    "loss_function = max_prob_class(0)\n",
    "\n",
    "output = model(tensor_final.unsqueeze(0))\n",
    "loss = loss_function(output)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for i in range(len(params_tot)):\n",
    "    print(params_tot[i].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tile_functions import Tile_Creator_Circle, Tile_Creator_Ellipse, Tile_Creator_Square, Tile_Creator_Rectangle, Tile_Creator_Triangle, Tile_Creator_Trapezoid, Tile_Creator\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import fnmatch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from loss_functions import *\n",
    "from dataset_functions import *\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import scipy \n",
    "# Transforming from PIL to Tensor\n",
    "transform1 = transforms.ToTensor()\n",
    "\n",
    "# Transforming from Tensor to PIL\n",
    "transform2 = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_function = Tile_Creator_Circle()\n",
    "params = tile_function.Params_Creator()\n",
    "print(params)\n",
    "tile, color, mask = tile_function(64,params)\n",
    "tile_PIL = transform2(tile)\n",
    "tile_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_creator = Tile_Creator([3,0,0,0,0,0])\n",
    "params = tile_creator.Params_Creator()\n",
    "print(params)\n",
    "tile, color, mask_tot = tile_creator(64,params)\n",
    "tile_PIL = transform2(tile)\n",
    "tile_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(mask_tot)\n",
    "mask_tot.data.clamp_(0, 1)\n",
    "torch.max(mask_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to apply the perlin noise as background\n",
    "from perlin_noise import Perlin_Noise_Creator, Inverted_Perlin_Noise_Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = params[1]\n",
    "perlin_noise = Inverted_Perlin_Noise_Creator(64,color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image = tile*mask_tot + perlin_noise*(1-mask_tot)\n",
    "final_image_PIL = transform2(final_image)\n",
    "final_image_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove con la funzione frattale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_functions import Fractal_Patch_Generator, PatchApplierMask\n",
    "import patch_config_mask as patch_config_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'perlin_noise'\n",
    "config = patch_config_mask.patch_configs[mode]()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.list_of_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_class = Tile_Creator(config.list_of_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_function = Fractal_Patch_Generator(config.dim_tile, config.dim_patch, config.mul_fact, tile_class, config.rotation_mode, config.BackgroundStyle, config.mask_function).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tile_class.Params_Creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_function.populate(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_PIL = transform2(gen_function.patches[3])\n",
    "tile_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_PIL = transform2(gen_function.masks[3])\n",
    "mask_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch, mask_attack = gen_function.application()\n",
    "adv_patch_PIL = transform2(adv_patch)\n",
    "adv_patch_PIL\n",
    "# mask_attack_PIL = transform2(mask_attack)\n",
    "# mask_attack_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringa = '0_ghost.pt'\n",
    "stringa[:-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the images and the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img): # this method for taking a non-square image and make it square by filling the difference in w and h with gray\n",
    "                                       # needed to keep proportions\n",
    "        h, w = img.size\n",
    "        if w==h:\n",
    "            padded_img = img\n",
    "        else:\n",
    "            dim_to_pad = 1 if w<h else 2\n",
    "            if dim_to_pad == 1:\n",
    "                padding = int((h - w) / 2)\n",
    "                padded_img = Image.new('RGB', (h,h), color=(0,0,0))\n",
    "                padded_img.paste(img, (0, int(padding)))\n",
    "            else:\n",
    "                padding = (w - h) / 2\n",
    "                padded_img = Image.new('RGB', (w, w), color=(0,0,0))\n",
    "                padded_img.paste(img, (int(padding), 0))\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images to pad\n",
    "images = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\data\\images\")]\n",
    "\n",
    "for image in images:\n",
    "    image_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\images' + '/' + image\n",
    "    save_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\images_pad' + '/' + image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_pad = pad(image)\n",
    "    image_pad.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks to pad\n",
    "images = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks\")]\n",
    "\n",
    "for image in images:\n",
    "    image_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks' + '/' + image\n",
    "    save_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad' + '/' + image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_pad = pad(image)\n",
    "    image_pad.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the masks as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks to pad\n",
    "images = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad\")]\n",
    "\n",
    "for img in images:\n",
    "    image_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad' + '/' + img\n",
    "    output_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad_tensors' + '/' + img[:-4] + '.pt'\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform1(image)\n",
    "    torch.save(image_tensor, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if they are correct \n",
    "# Masks to pad\n",
    "images = [f for f in os.listdir(r\"C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad_tensors\")]\n",
    "count = 0\n",
    "for img in images:\n",
    "    print(img)\n",
    "    count +=1\n",
    "    image_path = r'C:\\Users\\derea\\OneDrive\\Desktop\\data\\masks_pad_tensors' + '/' + img\n",
    "    image_tensor = torch.load(image_path)\n",
    "    image_PIL = transform2(image_tensor)\n",
    "    image_PIL.show()\n",
    "    if count == 5:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime import image\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import fnmatch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from patch_functions import *\n",
    "from loss_functions import *\n",
    "from dataset_functions import *\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import scipy\n",
    "\n",
    "# Transforming from PIL to Tensor\n",
    "transform1 = transforms.ToTensor()\n",
    "\n",
    "# Transforming from Tensor to PIL\n",
    "transform2 = transforms.ToPILImage()\n",
    "\n",
    "images = [f for f in os.listdir('/home/andread98/yolov3/MyWork/data_mask') if f.endswith('.jpeg')]\n",
    "for img in images:\n",
    "    image_path = '/home/andread98/yolov3/MyWork/data_mask/' + img\n",
    "    print(image_path)\n",
    "    mask_path = '/home/andread98/yolov3/MyWork/data_mask/mask/' + img[:-5] + '.pt'\n",
    "    print(mask_path)\n",
    "    image_PIL = Image.open(image_path).convert('RGB')\n",
    "    h, w = image_PIL.size\n",
    "    print(h,w)\n",
    "    mask_tensor = torch.load(mask_path)\n",
    "    print(mask_tensor.shape)\n",
    "    image_tensor = transform1(image_PIL)\n",
    "    print(image_tensor.shape)\n",
    "    random_attack = torch.ones((3,w,h))\n",
    "    image_final_tensor = random_attack*mask_tensor + image_tensor*(1 - mask_tensor)\n",
    "    image_final_tensor_PIL = transform2(image_final_tensor)\n",
    "    image_final_tensor_PIL.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime import image\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import fnmatch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from patch_functions import *\n",
    "from loss_functions import *\n",
    "from dataset_functions import *\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import scipy\n",
    "\n",
    "# Transforming from PIL to Tensor\n",
    "transform1 = transforms.ToTensor()\n",
    "\n",
    "# Transforming from Tensor to PIL\n",
    "transform2 = transforms.ToPILImage()\n",
    "\n",
    "# images = [f for f in os.listdir('/home/andread98/yolov3/MyWork/data_mask') if f.endswith('.jpeg')]\n",
    "# for img in images:\n",
    "#     image_path = '/home/andread98/yolov3/MyWork/data_mask/' + img\n",
    "#     print(image_path)\n",
    "#     mask_path = '/home/andread98/yolov3/MyWork/data_mask/mask/' + img[:-5] + '.pt'\n",
    "#     print(mask_path)\n",
    "#     image_PIL = Image.open(image_path).convert('RGB')\n",
    "#     h, w = image_PIL.size\n",
    "#     print(h,w)\n",
    "#     mask_tensor = torch.load(mask_path)\n",
    "#     print(mask_tensor.shape)\n",
    "#     image_tensor = transform1(image_PIL)\n",
    "#     print(image_tensor.shape)\n",
    "#     random_attack = torch.ones((3,w,h))\n",
    "#     image_final_tensor = random_attack*mask_tensor + image_tensor*(1 - mask_tensor)\n",
    "#     image_final_tensor_PIL = transform2(image_final_tensor)\n",
    "#     image_final_tensor_PIL.show() \n",
    "    \n",
    "# Masks to pad\n",
    "images = [f for f in os.listdir('/home/andread98/yolov3/MyWork/data_mask') if f.endswith('.jpeg')]\n",
    "\n",
    "for image in images:\n",
    "    start_path = '/home/andread98/yolov3/MyWork/data_mask' + '/' + image\n",
    "    end_path = '/home/andread98/yolov3/MyWork/data_mask' + '/' + image[:-5] + '.jpg'\n",
    "    os.save(start_path,end_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derea\\anaconda3\\envs\\yolov3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from email.mime import image\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import fnmatch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from patch_functions import *\n",
    "from loss_functions import *\n",
    "from dataset_functions import *\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import scipy\n",
    "\n",
    "# Transforming from PIL to Tensor\n",
    "transform1 = transforms.ToTensor()\n",
    "\n",
    "# Transforming from Tensor to PIL\n",
    "transform2 = transforms.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_functions import Fractal_Patch_Generator, PatchApplierMask\n",
    "from tile_functions import Tile_Creator, Tile_Creator_Circle\n",
    "import patch_config_mask as patch_config_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0]\n",
      "[tensor([0.2637, 0.7274]), tensor([0.9889, 0.0450]), tensor([0.0019, 0.0013])]\n",
      "tensor(0.7000, requires_grad=True) tensor(0.3300, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAMMUlEQVR4nO2dW3IbxxWGv547BjeRlCWnIinZQNbgKnsLyZOfvBIvxU9+SrbgB28hK0hkpaKSRNK4DebeeeiexgAEgRkQwIBV+YulAgmi58w//zl9+vRpSsgff+T/aAYHkF0b0Raio+taHV77MHRordPdpQ/H+V1BPSHHfCNBYp3djBYQlN06wZqyBOW/+XNHluzBn/jXxk/OY2r9uiuylLje8f4nfjiDEa3wAz+xLVr9nb+e9Lp/4x/1bzdjlqD8jl9+5vuTGtEK3/OzoHzs3Yj+ia4bstj4yYYbanF9zccPvDmREa3whg/veM/OSfAU8X7r5bZEdCUuh1zqqN/Zl0P+Hb/skNWZsUmWYlSJq9upR8DXfNwrq3Nie65gxHVma+o4SFbiyV+7sNUNoWtxXaCseExZdC2uS4tWCo+5IWwR19NF3sgLLlNW7FAWW8R16jWZHv8yZcVON4QuItfFyordyuKs4rp0WbHPDeG84rpkWbFXWZxJXM9AVjRwQziXuC5cVjRRFicX1/OQFc3cEE4vrsuXFQ2VxQnF9WxkRWM3hFOK61nIiubK4iTiek6yoo0bwmnE9VxkRStlcWRxPTNZ0dIN4djiekayoq2yOJq4np+saO+GcDxxPS9ZcYCyOIK4nqWsaE6W2Z5SeLq46rJ6OP5lYj9ZW+/hCeLaI6unUfb0vcpd2EPWjk8/RVwbsmp13Q7xKFl7eT5IXC2i1QV65Raymlt5mLj2yqqtMafaZXqAzS6aVg/TtNxkehy5Mwc4fBLcMe7DXpfTYY2sA2RvWm4afraVrOrYytdG/9SpsSLrsADRWFxHyK02hn7YC3hqWMaOg/G2ceRSsnrbXlYG3YZ868kWCAv57Z5pcSWrb/nF2hPa9kCepY1ga7y3DmVKgJAIiSgRb/jwep+4BLzm4xs+lNUHd848u9CVvg5YGyqakAiJVWIV2CX2N/xqbxeXfm2Tf8OvJXaBXWJJLImQ1YBtjeiEr1aHBgQ68RF1pgrsHPuK+1d8/i9/2HobAl7x+Yr7iJ5DYVPYFBaloBRIgRSarxYkPMmZD0JzsrQKSkSJVWLn2BluhpfgJfgp3l/45yde5dhAdS9GVoV61yP1SXxSl9QlcygsCovSQqIpu1y+GpJlwpNVYOW4KV6Mv6QXEUaES8KYAOSYyS3XDz8/ZjJhPOFFQNwjColCoh7LgMQjdchsSosSpICL5WsvWXVB2RlOihfTW9CfMZgxnDOI6C8JUvwC+zUff+dFoUOhkVX5mo/v+ZNN4ZH0iEMWA+ZDZkPmfRYBS4/UJbco1CfbUnYe7CZLB3IVmzK8mGBOf8p4wnjCaM4wIkzwU9wCp8TKcPvMp4zqo/SZf+ZVRGhR2uQemU8SEg2YjZmOmYyYDFgExC6pimWtXPJs4tpB1oqpHCfFXxBOGd1zdc/VhPGcQUyQ4uU4JbZKCDLcP/JhzqCs5lmL8obb//CmRAikhZdQLAkienP6cwZz+hG9K+5HTPtEHolD3pav8+AxsgxTdo4T488Z/M7VLTd3XE8YLwgT/Ay31HmA8VZuuemzmDFUA/VZ3HKTV4uhEgR2gZPhpngJfkwvppcQZHgF9wNEoPkqmvN1HnFtJ6uuqSXBjOEd11/46pabCaOIMMVTfldWaZexdsHgLe8X9EssJavfeCf1u0IiBaJEWHpKVay5KV6Gm2vqZz1iB1kL+ftxBr62kiWomIrxZwxvufnEqy+8nDJe0ktxC+wqq9TJl0GBdVeJq8/ijptiLfXVrJVIWUtECpwcu8BSAwpkgHTIbb3kvghnfEiWXsEU2Cn+nMEd15949ZmvJoyX9LIVU1Sa2sSc/jt+W9C/4e49bzferT4iJAhss2YqK+oFUmVeKl+1Gk+OpxbXBlmrUJXhLQh/5+oLX33h5YTxkrBiSshHaFIosW657rO45bp8ZEVV81yrqK0TBdKmdKoU30KKNsHrpNiirBIrw4kJpoxuubnlZlrTVC1I7cKC/jt+eyirDdTkqfL+noV0KFwyt8q8BNK+jIp8nSwhoUQUWCnenP49V3dcTxi1ZYpKXI/Jqg4JAlEC2OAu6U0YeaQ+sU/ikNkUQktsv7hO6okbZWUV192Y3pSxyqciwiqiN2VKYdH4jKniS0KBneJGhBPGAcuQZUDiklvVervNrR0f5smvolWKt6A/YazyqRSvNve1QBNZGZiaT6ENCCsD+ileqQMlZy80rGFtUpdYOXaMP2MwYTRnkOAXOGruO/VjrTaFrQInwZ8zmDCaMYjxc2yJ1ZCp09m5qsFLHdrdJb0ZwznDmCDDLXXuc55HKowZMcGc4Yyhiphle2kfHWaRLyRCrZYjwjkDlaabRd95rKyemY4GxpJMh4LDK9FHwUpZKhFN8CLCiH6Cn+OUjcVfw8Pfbz2CWj8k+BH9iDDBK/Rj6xJmd0cHrAR/SbgkSHFNWG1soqi9EOsvaB5xalONuyRYEia1sNWipeLY2FCWk+BXlTynbDcBiQdZjtj57q6hjD0p/pIgWbOnM1hmc6yqW3kpfoZbtIvrTbhoy5cosDLcFL+qmq2sbTbIkaGTUlm5YYaXrkolzX2wIQtNi1OsMmQ7xc3w8irXa2TOaWDI0gE+w1HPsKWsmqP5lpcwNbUM59ICvDD7gLVJsAkRbW+hEVPq37pJbddbR8dmBl/fK+72MZpp0VjV7VqH9Qy+XjC5IBirOn9+mqzaHNO05n02GKu6TN6BdTeUgrLWf9CxZRU7K6u6FtbqT2+qandpWjZWM/h+PH099BD66nWThO6H6AwrN7SQNoVL7lTFtsZPslUwaTGsen4OuUtuU1hdhwhDlhSUDoVL6unmllbO2HA3oVHuahzQonQoPDKX1KEQ+hF2BsvMhOoZeqQeiUtmtxbXXr6a/M7ql9U2j0vm6U6bvAoOnc2KG26Y+yQ9Yo/EJrfaNQDt5qItU9oe1XXjr9nTGeoBvnQofJIeUY/YI1PbUG2mRVl7IR+ooBFTNR8sPLIecY/IJ1Fu2DDAn4jTzQDvk4ZEIQu/1s3ScsyHv996BBUTfJKQRUjk61akC1AWekpWs2EaEg2Yh0QeqaVNPJOVlaaUrFaWqL4t0WnAYj2DlxalS9ZjOWQ2YBYQu2Qt04gnQhozAuIBsyGzHktjxiUoS0GHrYBkyHzMdMBcRVb157JPbWgVrUo1zwyYj5kOmQe1gNVwnBPB7EhLoTvzCo+0z2LMZE4/rm1D0bgEfgDMysbWBkRjJmMmfRYqGoh2U/NJsPG3lVVkzQKWIyYRvZie2bkodJPQ8fky6y2bwiMLicZMrrgfMQlYOqtQ0DHqZEmBsJA2pUc6YHHFfUJQNfkpL7HLY/OlmKoWW1mP5ZjpNXdX3A9YeKS27gqhiazO2Z8FOr7mAfGIabbqXjRRyz5i2avyvhVTIyY33N5wO2IaELsHpi8nwQZZuq3HonBJ+0QF9/XuReiBW2CbreyDb6JWpSprmpq85MtLPr/gvk/kXky0UtiiLBXJVYP/AFHW+jwt5HpPKequW91HVduXtYiuNfWSL6/4dM3dgLlHUiWiTROXU8/XWxtwJaBy6IBEMoNV9+LWbmVj6o57qu5EmiBVnSFIQ6Ix0xtuX/L5mrshs2Bt/dC9phS2t3ZX91M65D1iUy1xyTzSR/rg6+WXjWcsWfM71V9buGQ+icoSrrm74fYF9wOdWOVts9Az5KuPHRpYBS8HGaxmq9QnDlg+dsKi2m2URgw1QUmzlFG1oIB4wFxlCQ9PWLQKVefJ7HccRzF8lQ55tQpRi9vlgMXDszuF3rNa291br1k3PbtzOUG9jt0HnTRfqhBYiSILSPoshkw3ToWZIxJlbZ+9cjrjxXtPhZlCe1OmzrZg3HuETkcNC4Tusi4qfS1GTM15Q8VXipvjmLnSZAYOuVcxteO8oWgz950fDQ9n6kdtVxtTjq5hLofM1UlWdZg1w6v6EtQGst6eccld0uoY646TrLRl6px1iObHfk0IW22aueQFaQ8714dv1Pkbc15cK8vS4ioccpv8kTPS+iqtrD9zxabVgXKVAQiBlNUeh01RIjysUqevVnUER8i16K4lWb2QxuMOo4ku9oAP+C/7NGVVfVUIgNK04tTT+noiWqOm/i+HRahOqoBO8y2XdZg8U9Gm7lys3qtBbPvUU6J4V/XS/wFO6fCvrMp8XAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100 at 0x25F8F35C2E0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mode = 'perlin_noise'\n",
    "config = patch_config_mask.patch_configs[mode]()\n",
    "print(config.list_of_shape)\n",
    "tile_creator = Tile_Creator(config.list_of_shape)\n",
    "print(tile_creator.centroids)\n",
    "params = tile_creator.Params_Creator()\n",
    "params = tile_creator.Params_Clamp(params)\n",
    "print(params[4], params[5])\n",
    "tile, color, mask = tile_creator(100,params)\n",
    "tensor_PIL = transform2(tile)\n",
    "tensor_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(config.list_of_shape)\n",
    "tile_class = Tile_Creator(config.list_of_shape)\n",
    "\n",
    "gen_function = Fractal_Patch_Generator(config.dim_tile, config.dim_patch, config.mul_fact, tile_class, config.rotation_mode, config.BackgroundStyle, config.mask_function).to(device)\n",
    "params = tile_class.Params_Creator()\n",
    "gen_function.populate(params)\n",
    "\n",
    "\n",
    "# d_tile = torch.autograd.grad(torch.mean(gen_function.patches[0]), params, create_graph=True)\n",
    "# print(d_tile)\n",
    "\n",
    "adv_patch, mask_attack = gen_function.application()\n",
    "\n",
    "# d_patch = torch.autograd.grad(torch.mean(adv_patch), params)\n",
    "# print(d_patch)\n",
    "\n",
    "adv_patch_PIL = transform2(adv_patch)\n",
    "adv_patch_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "data = np.random.uniform(low=0,high=1,size=(500,2))\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\t\t\n",
    "# function to compute euclidean distance\n",
    "def distance(p1, p2):\n",
    "\treturn np.sum((p1 - p2)**2)\n",
    "\n",
    "# initialization algorithm\n",
    "def initialize(data, k):\n",
    "\t'''\n",
    "\tinitialized the centroids for K-means++\n",
    "\tinputs:\n",
    "\t\tdata - numpy array of data points having shape (200, 2)\n",
    "\t\tk - number of clusters\n",
    "\t'''\n",
    "\t## initialize the centroids list and add\n",
    "\t## a randomly selected data point to the list\n",
    "\tcentroids = []\n",
    "\tcentroids.append(data[np.random.randint(\n",
    "\t\t\tdata.shape[0]), :])\n",
    "\n",
    "\t## compute remaining k - 1 centroids\n",
    "\tfor c_id in range(k - 1):\n",
    "\t\t\n",
    "\t\t## initialize a list to store distances of data\n",
    "\t\t## points from nearest centroid\n",
    "\t\tdist = []\n",
    "\t\tfor i in range(data.shape[0]):\n",
    "\t\t\tpoint = data[i, :]\n",
    "\t\t\td = sys.maxsize\n",
    "\t\t\t\n",
    "\t\t\t## compute distance of 'point' from each of the previously\n",
    "\t\t\t## selected centroid and store the minimum distance\n",
    "\t\t\tfor j in range(len(centroids)):\n",
    "\t\t\t\ttemp_dist = distance(point, centroids[j])\n",
    "\t\t\t\td = min(d, temp_dist)\n",
    "\t\t\tdist.append(d)\n",
    "\t\t\t\n",
    "\t\t## select data point with maximum distance as our next centroid\n",
    "\t\tdist = np.array(dist)\n",
    "\t\tnext_centroid = data[np.argmax(dist), :]\n",
    "\t\tcentroids.append(next_centroid)\n",
    "\t\tdist = []\n",
    "\treturn centroids\n",
    "\n",
    "# call the initialize function to get the centroids\n",
    "centroids = initialize(data, k )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = [torch.from_numpy(item).float() for item in centroids]\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_list = [1,2,3,4]\n",
    "sum(dummy_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c08e6398de8ae9d67f7cdc2c67bb88665afbf274ad7155eac4ebf01ca5113666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
